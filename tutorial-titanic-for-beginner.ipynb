{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## [캐글 필사하기] 타이타닉 for beginner\n\n본 커널은 [@subinium 님의 자료](http://www.kaggle.com/code/subinium/subinium-tutorial-titanic-beginner/notebook)를 참고하여 필사한 자료입니다. 감사합니다 수빈님🙇🏻‍♂️","metadata":{}},{"cell_type":"markdown","source":"### 1. 라이브러리 불러오기\n\n우선 코드를 작성하기에 앞서 기초적으로 필요한 라이브러리를 불러옵니다.","metadata":{}},{"cell_type":"code","source":"# 데이터 분석 관련\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport numpy as np\n\n# 데이터 시각화 관련\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid') # matplotlib 스타일\n\n# 그래프 출력에 필요한 IPython 명령어\n%matplotlib inline\n\n# Scikit-Learn의 다양한 머신러닝 모듈을 불러오기\n# 분류 알고리즘 중에서 로지스틱 회귀, SVC & LinearSVC, 랜덤포레스트, K-최근접이웃 알고리즘을 사용할 예정\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:17.419433Z","iopub.execute_input":"2023-09-16T12:55:17.420043Z","iopub.status.idle":"2023-09-16T12:55:19.269340Z","shell.execute_reply.started":"2023-09-16T12:55:17.419987Z","shell.execute_reply":"2023-09-16T12:55:19.268332Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 2. 데이터 읽기\n\nKaggle 또는 데이터 분석에서 가장 많이 사용되는 파일 형식은 `csv` 파일입니다.\n\n코드로 데이터를 읽는 방법은 다양한 방법이 있지만, 그 중에서도 가장 유용한 것은 `pd.read_csv`로 읽는 방법입니다.","metadata":{}},{"cell_type":"code","source":"# 데이터 가져오기\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# 데이터 미리보기\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.271241Z","iopub.execute_input":"2023-09-16T12:55:19.271668Z","iopub.status.idle":"2023-09-16T12:55:19.323966Z","shell.execute_reply.started":"2023-09-16T12:55:19.271641Z","shell.execute_reply":"2023-09-16T12:55:19.323094Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"데이터의 정보는 `info` 메서드로 확인할 수 있습니다.\n\n훈련 데이터와 테스트 데이터를 확인해보도록 하겠습니다.","metadata":{}},{"cell_type":"code","source":"train_df.info()\nprint()\nprint('-' * 20)\nprint()\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.325481Z","iopub.execute_input":"2023-09-16T12:55:19.326160Z","iopub.status.idle":"2023-09-16T12:55:19.360228Z","shell.execute_reply.started":"2023-09-16T12:55:19.326121Z","shell.execute_reply":"2023-09-16T12:55:19.359002Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n--------------------\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"위 결과에서 각각의 데이터 개수는 891개, 418개인 것을 확인할 수 있습니다.\n\n특성(feature)는 각각 12개, 11개인데, 그 이유는 훈련 데이터의 생존 여부(Survived)를 알고 있기 때문입니다.\n\n여기서 주의깊게 봐야할 부분은 다음과 같습니다.\n- 각 데이터는 빈 부분이 있는가?\n    - 빈 부분이 있다면, `drop` 할 것인가 or `default` 값으로 채워 넣을 것인가\n    - `Cabin`, `Age`, `Embarked` 세 항목에 주의\n- 데이터는 float64로 변환할 수 있는가?\n    - 아니라면, 범주형 데이터로 만들 수 있는가\n\n필요 없는 부분이라고 생각되는 부분을 지웁니다. 여기서는 `PassengerId`, `Name` 그리고 `Ticket`을 지웁니다. `Name`과 `Ticket`에서 가져올 수 있는 데이터는 없기 때문입니다.\n\n하지만, 이 문제에서 결과물은 `PassengerId`, `Survived` 요소가 필요하므로 훈련 데이터에서만 삭제합니다.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\ntest_df = test_df.drop(['Name', 'Ticket'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.361911Z","iopub.execute_input":"2023-09-16T12:55:19.362247Z","iopub.status.idle":"2023-09-16T12:55:19.373026Z","shell.execute_reply.started":"2023-09-16T12:55:19.362217Z","shell.execute_reply":"2023-09-16T12:55:19.371875Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 3. 데이터 하나하나 처리하기\n\n이제 남은 데이터 종류는 다음과 같습니다.\n1. Pclass\n2. Sex\n3. Age\n4. SibSp\n5. Parch\n6. Fare\n7. Cabin\n8. Embarked\n\n순서대로 보도록 하겠습니다.","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Pclass\n\nPclass는 1등석, 2등석, 3등석과 같은 정보인 서수형(순서형) 데이터입니다. 처음 확인 시에 데이터가 비어있지 않은 것을 확인할 수 있었습니다.","metadata":{}},{"cell_type":"code","source":"train_df['Pclass'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.376901Z","iopub.execute_input":"2023-09-16T12:55:19.377235Z","iopub.status.idle":"2023-09-16T12:55:19.387927Z","shell.execute_reply.started":"2023-09-16T12:55:19.377207Z","shell.execute_reply":"2023-09-16T12:55:19.387039Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Pclass\n3    491\n1    216\n2    184\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"1, 2, 3은 정수이니, 그냥 실수로만 바꾸면 되지 않을까 생각할 수 있습니다. 하지만 1, 2, 3 등급은 경우에 따라 다를 수 있지만 연속적인 정보가 아니며, 각 차이 또한 균등하지 않습니다. 그렇기에 범주형(category) 데이터로 인식하고 인코딩해야합니다. (비슷한 예시로 영화 별점 등이 있습니다.)\n\n이 데이터는 범주형 데이터이므로 **one-hot-encoding**을 `pd.get_dummies()` 메서드로 인코딩합시다.","metadata":{}},{"cell_type":"code","source":"pclass_train_dummies = pd.get_dummies(train_df['Pclass'])\npclass_test_dummies = pd.get_dummies(test_df['Pclass'])\n\npclass_train_dummies.columns = ['First', 'Second', 'Third']\npclass_test_dummies.columns = ['First', 'Second', 'Third']\n\n# inplace = True : 기존 데이터프레임에 변경된 설정으로 덮어쓰겠다는 의미. 즉, 원본을 변경할지 여부\ntrain_df.drop(['Pclass'], axis=1, inplace=True)\ntest_df.drop(['Pclass'], axis=1, inplace=True)\n\ntrain_df = train_df.join(pclass_train_dummies)\ntest_df = test_df.join(pclass_test_dummies)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.389035Z","iopub.execute_input":"2023-09-16T12:55:19.389584Z","iopub.status.idle":"2023-09-16T12:55:19.404999Z","shell.execute_reply.started":"2023-09-16T12:55:19.389550Z","shell.execute_reply":"2023-09-16T12:55:19.404037Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"이렇게 Pclass의 원본을 날리고, 범주형 데이터로 데이터를 변환했습니다.","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Sex\n\nSex는 성별입니다. 남과 여로 나뉘므로 이 또한 **one-hot-encoding**을 진행해봅시다.","metadata":{}},{"cell_type":"code","source":"sex_train_dummies = pd.get_dummies(train_df['Sex'])\nsex_test_dummies = pd.get_dummies(test_df['Sex'])\n\nsex_train_dummies.columns = ['Female', 'Male']\nsex_test_dummies.columns = ['Female', 'Male']\n\ntrain_df.drop(['Sex'], axis=1, inplace=True)\ntest_df.drop(['Sex'], axis=1, inplace=True)\n\ntrain_df = train_df.join(sex_train_dummies)\ntest_df = test_df.join(sex_test_dummies)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.406102Z","iopub.execute_input":"2023-09-16T12:55:19.407142Z","iopub.status.idle":"2023-09-16T12:55:19.419423Z","shell.execute_reply.started":"2023-09-16T12:55:19.407107Z","shell.execute_reply":"2023-09-16T12:55:19.418249Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Age\n\n나이는 연속형 데이터이므로, 큰 처리가 필요없습니다. (카테고리화를 하여 일부 알고리즘에 더 유용한 결과를 만들 수 있습니다.) 하지만, 일부 NaN 데이터가 있으니 이를 채울 수 있는 방법에 대해서 생각해봅시다.\n\n1. 랜덤\n2. 평균값\n3. 중간값\n4. 데이터 버리기\n\n저는 평균값으로 채우도록 하겠습니다. 데이터의 통일성을 갖기 위해 훈련 데이터셋의 평균값으로 훈련, 테스트 데이터셋을 채우겠습니다.","metadata":{}},{"cell_type":"code","source":"train_df[\"Age\"].fillna(train_df[\"Age\"].mean(), inplace=True)\ntest_df[\"Age\"].fillna(train_df[\"Age\"].mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.420935Z","iopub.execute_input":"2023-09-16T12:55:19.421305Z","iopub.status.idle":"2023-09-16T12:55:19.435071Z","shell.execute_reply.started":"2023-09-16T12:55:19.421273Z","shell.execute_reply":"2023-09-16T12:55:19.433880Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### 3.4 SibSp & Panch\n\n형제 자매와 부모님은 가족으로 함께 처리할 수 있습니다. 하지만 마찬가지로 바꿀 필요는 없습니다.","metadata":{}},{"cell_type":"markdown","source":"### 3.5 Fare\n\nFare은 탑승료입니다. 우선 빈 부분을 `fillna` 메서드로 채우겠습니다.\n\n저는 데이터 누락이 아닌 무단 탑승이라 생각하고 0으로 입력하겠습니다.","metadata":{}},{"cell_type":"code","source":"test_df[\"Fare\"].fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.436865Z","iopub.execute_input":"2023-09-16T12:55:19.437246Z","iopub.status.idle":"2023-09-16T12:55:19.447007Z","shell.execute_reply.started":"2023-09-16T12:55:19.437216Z","shell.execute_reply":"2023-09-16T12:55:19.445726Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 3.6 Cabin\n\nCabin은 객실입니다. NaN이 대부분인 데이터이므로 버립시다. 이 데이터를 살리는 것은 너무 어려운 일입니다.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.448332Z","iopub.execute_input":"2023-09-16T12:55:19.450147Z","iopub.status.idle":"2023-09-16T12:55:19.462445Z","shell.execute_reply.started":"2023-09-16T12:55:19.450049Z","shell.execute_reply":"2023-09-16T12:55:19.460873Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 3.7 Embarked\n\nEmbarked는 탑승 항구를 의미합니다. 우선 데이터를 확인해보겠습니다.","metadata":{}},{"cell_type":"code","source":"train_df['Embarked'].value_counts()\ntest_df['Embarked'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.466411Z","iopub.execute_input":"2023-09-16T12:55:19.467599Z","iopub.status.idle":"2023-09-16T12:55:19.477718Z","shell.execute_reply.started":"2023-09-16T12:55:19.467566Z","shell.execute_reply":"2023-09-16T12:55:19.476957Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Embarked\nS    270\nC    102\nQ     46\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"S가 대다수이고 train_df의 전체 데이터 개수는 891인데, 'Embarked'에 해당하는 train_df의 데이터 개수 합이 889(644 + 168 + 77)입니다. 따라서, 빈 부분은 S로 채우고 시작합시다.","metadata":{}},{"cell_type":"code","source":"train_df[\"Embarked\"].fillna('S', inplace=True)\ntest_df[\"Embarked\"].fillna('S', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.479115Z","iopub.execute_input":"2023-09-16T12:55:19.479625Z","iopub.status.idle":"2023-09-16T12:55:19.486145Z","shell.execute_reply.started":"2023-09-16T12:55:19.479586Z","shell.execute_reply":"2023-09-16T12:55:19.485101Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"'S', 'C', 'Q'로 나뉘므로 이 또한 **one-hot-encoding**을 진행해봅시다.","metadata":{}},{"cell_type":"code","source":"embarked_train_dummies = pd.get_dummies(train_df['Embarked'])\nembarked_test_dummies = pd.get_dummies(test_df['Embarked'])\n\nembarked_train_dummies.columns = ['S', 'C', 'Q']\nembarked_test_dummies.columns = ['S', 'C', 'Q']\n\ntrain_df.drop(['Embarked'], axis=1, inplace=True)\ntest_df.drop(['Embarked'], axis=1, inplace=True)\n\ntrain_df = train_df.join(embarked_train_dummies)\ntest_df = test_df.join(embarked_test_dummies)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.487326Z","iopub.execute_input":"2023-09-16T12:55:19.488105Z","iopub.status.idle":"2023-09-16T12:55:19.504902Z","shell.execute_reply.started":"2023-09-16T12:55:19.488076Z","shell.execute_reply":"2023-09-16T12:55:19.504044Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## 4. 데이터 나누기\n\n이제 학습용 데이터를 위해 데이터를 나누어야합니다.\n\n`(정보, 생존 여부)`와 같은 형태를 위하여 다음과 같이 데이터를 나눕니다.","metadata":{}},{"cell_type":"code","source":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test = test_df.drop(\"PassengerId\", axis=1).copy()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.508645Z","iopub.execute_input":"2023-09-16T12:55:19.509011Z","iopub.status.idle":"2023-09-16T12:55:19.517222Z","shell.execute_reply.started":"2023-09-16T12:55:19.508981Z","shell.execute_reply":"2023-09-16T12:55:19.516149Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## 5. 머신러닝 알고리즘 적용하기\n\n이제 로지스틱 회귀,SVC, 랜덤 포레스트, K-최근접이웃 알고리즘을 각각 적용해봅시다","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\n## Binary한 데이터가 주어질 때는 Linear Regression을 사용하는 것보다는 Logistic Regression을 사용하는 것이 훨씬 정확도가 높다.\n\nlogreg = LogisticRegression(max_iter=1000)\nlogreg.fit(X_train, Y_train)\n                       \nlogreg.score(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.519381Z","iopub.execute_input":"2023-09-16T12:55:19.519708Z","iopub.status.idle":"2023-09-16T12:55:19.686739Z","shell.execute_reply.started":"2023-09-16T12:55:19.519680Z","shell.execute_reply":"2023-09-16T12:55:19.685523Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0.8047138047138047"},"metadata":{}}]},{"cell_type":"code","source":"# Support Verctor Machines\n## SVM은 패턴 인식, 분류 그리고 회귀 분석 등의 머신러닝 알고리즘이다.\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\n\nsvc.score(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.688837Z","iopub.execute_input":"2023-09-16T12:55:19.689598Z","iopub.status.idle":"2023-09-16T12:55:19.829331Z","shell.execute_reply.started":"2023-09-16T12:55:19.689556Z","shell.execute_reply":"2023-09-16T12:55:19.828208Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0.6868686868686869"},"metadata":{}}]},{"cell_type":"code","source":"# Random Forests\n## 기존 Bagging의 이점을 살리고 변수를 랜덤으로 선택하는 과정을 추가함으로써 개별 나무들의 상관성을 줄여 예측력을 향상한 앙상블 모형\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nrandom_forest.score(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:19.830566Z","iopub.execute_input":"2023-09-16T12:55:19.830942Z","iopub.status.idle":"2023-09-16T12:55:20.146880Z","shell.execute_reply.started":"2023-09-16T12:55:19.830912Z","shell.execute_reply":"2023-09-16T12:55:20.145743Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0.9820426487093153"},"metadata":{}}]},{"cell_type":"code","source":"# K-Nearest Neighbor\n## 주변의 가장 가까운 K개의 데이터를 보고 데이터가 속할 그룹을 판단하는 알고리즘\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, Y_train)\n\nknn.score(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:20.148199Z","iopub.execute_input":"2023-09-16T12:55:20.148538Z","iopub.status.idle":"2023-09-16T12:55:20.229515Z","shell.execute_reply.started":"2023-09-16T12:55:20.148509Z","shell.execute_reply":"2023-09-16T12:55:20.228254Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0.835016835016835"},"metadata":{}}]},{"cell_type":"markdown","source":"## 6. 제출용 파일 만들기\n\n**Random Forest**가 가장 좋은 결과를 내는 것을 알 수 있습니다. 그 결과로 submission 파일을 만들어 제출해봅시다.","metadata":{}},{"cell_type":"code","source":"# Random Forests\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\n\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_df[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\nsubmission.to_csv('titanic.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T12:55:20.231299Z","iopub.execute_input":"2023-09-16T12:55:20.231647Z","iopub.status.idle":"2023-09-16T12:55:20.562518Z","shell.execute_reply.started":"2023-09-16T12:55:20.231619Z","shell.execute_reply":"2023-09-16T12:55:20.561625Z"},"trusted":true},"execution_count":21,"outputs":[]}]}