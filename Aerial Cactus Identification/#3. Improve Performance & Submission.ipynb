{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/scottxchoo/cactus-3-improve-performance-submission?scriptVersionId=145052895\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Improve Performance & Submission\n\nIn Baseline, we used a simple CNN model. This time, we'll make four improvements to imporve performance.\n\n1. Perform different image transformations\n2. Create a deeper CNN model.\n3. Use a better optimizer.\n4. Increase the number of epochs in training.\n\nExcept for these four things, the code is similar to the baseline.\n\n[베이스라인](https://www.kaggle.com/code/scottxchoo/cactus-2-baseline-model)에서는 간단한 CNN 모델을 사용했습니다. 이번에는 다음 네 가지를 개선해 성능을 높여보겠습니다.\n\n1. 다양한 이미지 변환을 수행합니다.\n2. 더 깊은 CNN 모델을 만듭니다.\n3. 더 뛰어난 옵티마이저를 사용합니다.\n4. 훈련 시 에폭 수를 늘립니다.\n\n이상의 네 가지를 제외하고는 [베이스라인](https://www.kaggle.com/code/scottxchoo/cactus-2-baseline-model)과 코드가 비슷합니다.","metadata":{}},{"cell_type":"markdown","source":"먼저, 시드값 고정부터 'Prepare Data (데이터 준비)'의 '[2] Define dataset classes (데이터셋 클래스 정의)'까지 베이스라인과 동일하게 진행합니다.\n\n## 1. Fix the seed value and set up your GPU equipment (시드값 고정 및 GPU 장비 설정)\n\n### [1] Fix the seed value (시드값 고정)","metadata":{}},{"cell_type":"code","source":"import torch # 파이토치\nimport random\nimport numpy as np\nimport os\n\n# 시드값 고정\nseed = 50\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)                # 파이썬 난수 생성기 시드 고정\nnp.random.seed(seed)             # 넘파이 난수 생성기 시드 고정\ntorch.manual_seed(seed)          # 파이토치 난수 생성기 시드 고정 (CPU 사용 시)\ntorch.cuda.manual_seed(seed)     # 파이토치 난수 생성기 시드 고정 (GPU 사용 시)\ntorch.cuda.manual_seed_all(seed) # 파이토치 난수 생성기 시드 고정 (멀티GPU 사용 시)\ntorch.backends.cudnn.deterministic = True # 확정적 연산 사용\ntorch.backends.cudnn.benchmark = False    # 벤치마크 기능 해제\ntorch.backends.cudnn.enabled = False      # cudnn 사용 해제","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:33:59.152745Z","iopub.execute_input":"2023-09-29T12:33:59.153154Z","iopub.status.idle":"2023-09-29T12:34:01.740824Z","shell.execute_reply.started":"2023-09-29T12:33:59.15312Z","shell.execute_reply":"2023-09-29T12:34:01.739648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [2] Set up GPU equipment (GPU 장비 설정)","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:01.742621Z","iopub.execute_input":"2023-09-29T12:34:01.743312Z","iopub.status.idle":"2023-09-29T12:34:01.828324Z","shell.execute_reply.started":"2023-09-29T12:34:01.743275Z","shell.execute_reply":"2023-09-29T12:34:01.827253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:01.829632Z","iopub.execute_input":"2023-09-29T12:34:01.830172Z","iopub.status.idle":"2023-09-29T12:34:01.845132Z","shell.execute_reply.started":"2023-09-29T12:34:01.830142Z","shell.execute_reply":"2023-09-29T12:34:01.84392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Prepare Data (데이터 준비)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndata_path = '/kaggle/input/aerial-cactus-identification/'\n\nlabels = pd.read_csv(data_path + 'train.csv')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:01.848087Z","iopub.execute_input":"2023-09-29T12:34:01.848879Z","iopub.status.idle":"2023-09-29T12:34:02.409815Z","shell.execute_reply.started":"2023-09-29T12:34:01.848843Z","shell.execute_reply":"2023-09-29T12:34:02.408834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from zipfile import ZipFile\n\n# 훈련 이미지 데이터 압축 풀기\nwith ZipFile(data_path + 'train.zip') as zipper:\n    zipper.extractall()\n    \n# 테스트 이미지 데이터 압축 풀기\nwith ZipFile(data_path + 'test.zip') as zipper:\n    zipper.extractall()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:02.414476Z","iopub.execute_input":"2023-09-29T12:34:02.416739Z","iopub.status.idle":"2023-09-29T12:34:05.88248Z","shell.execute_reply.started":"2023-09-29T12:34:02.416705Z","shell.execute_reply":"2023-09-29T12:34:05.881581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [1] Separate training/validation data (훈련/검증 데이터 분리)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 훈련 데이터, 검증 데이터 분리\ntrain, valid = train_test_split(labels,\n                                test_size = 0.1, # (1)\n                                stratify = labels['has_cactus'], # (2)\n                                random_state = 50)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:05.883774Z","iopub.execute_input":"2023-09-29T12:34:05.884321Z","iopub.status.idle":"2023-09-29T12:34:06.568714Z","shell.execute_reply.started":"2023-09-29T12:34:05.884289Z","shell.execute_reply":"2023-09-29T12:34:06.567781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('훈련 데이터 개수:', len(train))\nprint('검증 데이터 개수:', len(valid))","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:06.570351Z","iopub.execute_input":"2023-09-29T12:34:06.571003Z","iopub.status.idle":"2023-09-29T12:34:06.580719Z","shell.execute_reply.started":"2023-09-29T12:34:06.570971Z","shell.execute_reply":"2023-09-29T12:34:06.579502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [2] Define dataset classes (데이터셋 클래스 정의)","metadata":{}},{"cell_type":"code","source":"import cv2 # OpenCV 라이브러리\nfrom torch.utils.data import Dataset # 데이터 생성을 위한 클래스","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:06.582733Z","iopub.execute_input":"2023-09-29T12:34:06.583544Z","iopub.status.idle":"2023-09-29T12:34:06.964223Z","shell.execute_reply.started":"2023-09-29T12:34:06.58349Z","shell.execute_reply":"2023-09-29T12:34:06.963228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    # (1) 초기화 메서드(생성자)\n    def __init__(self, df, img_dir = './', transform = None):\n        super().__init__() ## (2) 상속받은 Dataset의 생성자 호출\n        # (3) 전달받은 인수들 저장\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n    \n    # (4) 데이터셋 크기 반환 메서드\n    def __len__(self):\n        return len(self.df)\n    \n    # (5) 인덱스(idx)에 해당하는 데이터 반환 메서드\n    def __getitem__(self, idx):\n        img_id = self.df.iloc[idx, 0]                  # 이미지 ID\n        img_path = self.img_dir + img_id               # (6) 이미지 파일 경로\n        image = cv2.imread(img_path)                   # 이미지 파일 읽기 \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 이미지 색상 보정\n        label = self.df.iloc[idx, 1]                   # 이미지 레이블(타깃값)\n        \n        if self.transform is not None:\n            image = self.transform(image) # (7) 변환기가 있다면 이미지 변환\n        return image, label # (8)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:06.965458Z","iopub.execute_input":"2023-09-29T12:34:06.965964Z","iopub.status.idle":"2023-09-29T12:34:06.973698Z","shell.execute_reply.started":"2023-09-29T12:34:06.965933Z","shell.execute_reply":"2023-09-29T12:34:06.972883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [3] Image transformation and data augmentation (이미지 변환과 데이터 증강)\n\nWe mentioned earlier that you can apply image transforms when creating a dataset with the ImageDataset class. Examples of popular image transforms include edge removal, symmetry, rotation, resize, original, noise, color change, block, and blur.\n\nWhy do we transform images? The answer is simple: to generate more data. You'll be able to create 8 or more images from a single image. Deep learning models are generally more accurate the more training data they have, so more meaningful data is better. This practice of converting images to increase the number of data is called data augmentation.\n\n앞서 ImageDataset 클래스로 데이터셋을 만들 때 이미지 변환기를 적용할 수 있다고 했습니다. 많이 쓰이는 이미지 변환의 예로는 가장자리 제거, 대칭, 회전, 크기 변경, 원본, 노이즈, 색상 변경, 차단, 흐릿하게 등이 있습니다.\n\n이미지를 변환하는 이유는 무엇일까요? 바로 데이터를 더 많이 생성하기 위해서입니다. 하나의 이미지로 8개 넘는 이미지를 만들 수 있게 됩니다. 딥러닝 모델은 대체로 훈련 데이터가 많을수록 정확해지므로 의미 있는 데이터는 많을수록 좋습니다. 이렇게 이미지를 변환하여 데이터 수를 늘리는 방식을 데이터 증강(data augmentation)이라고 합니다.","metadata":{}},{"cell_type":"markdown","source":"### [4] Define image transforms (이미지 변환기 정의)\n\nWe'll define our own image transforms to augment our data. To improve performance, we'll utilize different image transforms, creating one for training data and another for validation and testing data. This is because it's good to adapt the model to different situations during training, but during evaluation and testing, it can become unpredictable if it deviates too much from the original image.\n\n데이터를 증강해줄 이미지 변환기를 직접 정의해보죠. 성능을 개선하기 위해 다양한 이미지 변환기를 활용할 텐데, 훈련 데이터용과 검증 및 테스트 데이터용을 따로 만듭니다. 훈련 시에는 모델을 다양한 상황에 적응시키는 게 좋지만, 평가 및 테스트 시에는 원본 이미지와 너무 달라지면 예측하기 어려워질 수 있기 때문입니다.","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms # 이미지 변환을 위한 모듈\n\n# 훈련 데이터용 변환기\ntransform_train = transforms.Compose([transforms.ToTensor(), # (1)\n                                     transforms.Pad(32, padding_mode = 'symmetric'), # (2)\n                                     transforms.RandomHorizontalFlip(), # (3)\n                                     transforms.RandomVerticalFlip(), # (4)\n                                     transforms.RandomRotation(10), # (5)\n                                     transforms.Normalize((0.485, 0.456, 0.406), # (6)\n                                                          (0.229, 0.224, 0.225))])\n\n# 검증 및 테스트 데이터용 변환기\ntransform_test = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Pad(32, padding_mode = 'symmetric'),\n                                     transforms.Normalize((0.485, 0.456, 0.406),\n                                                          (0.229, 0.224, 0.225))])","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:06.977407Z","iopub.execute_input":"2023-09-29T12:34:06.977672Z","iopub.status.idle":"2023-09-29T12:34:07.480279Z","shell.execute_reply.started":"2023-09-29T12:34:06.977651Z","shell.execute_reply":"2023-09-29T12:34:07.479448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have combined multiple transforms into one with transforms.Compose().\n\n(6) transforms.Normalize() : Normalizes the data to the specified mean and variance. You can set them to values between 0 and 1. Here, I normalized the mean to (0.485, 0.456, 0.406) and the variance to (0.229, 0.224, 0.225).\n\nTwo questions arise here.\n\nFirst, why do we have three means and three variances?\nThe colors in the image data are red (R), green (G), and blue (B). We need to normalize red, green, and blue separately, which is why the mean and variance have three values.\n\nSecond, so why is the mean (0.485, 0.456, 0.406) and the variance (0.229, 0.224, 0.225)?\nOther values are fine, but when dealing with images, we usually normalize to these values. These values come from data from ImageNet, which has over a million images. I could calculate the mean and variance directly from the images I'm using, but it's a hassle, so I usually just use these values.\n\ntransforms.Compose()로 여러 변환기를 하나로 묶었습니다.\n\n(6) transforms.Normalize() : 데이터를 지정한 평균과 분산에 맞게 정규화해줍니다. 0~1 사이 값으로 설정해주면 되는데, 여기서는 평균을 (0.485, 0.456, 0.406)으로, 분산을 (0.229, 0.224, 0.225)로 정규화했습니다.\n\n여기서 두 가지 의문이 듭니다.\n\n첫째, 왜 평균과 분산이 각각 세 개씩 있을까요?\n이미지 데이터의 색상읜 빨강(R), 초록(G), 파랑(B)으로 구성돼 있습니다. 빨강, 초록, 파랑을 각각 정규화해야 해서 평균과 분선에 값을 세 개씩 전달한 겁니다.\n\n둘째, 그렇다면 왜 평균은 (0.485, 0.456, 0.406)이고 분산은 (0.229, 0.224, 0.225)일까요?\n다른 값을 해도 상관없지만 이미지를 다룰 때는 보통 이 값들로 정규화합니다. 이 값들은 백만 개 이상의 이미지를 보유한 이미지넷(ImageNet)의 데이터로부터 얻은 값입니다. 내가 사용할 이미지들로부터 평균과 분산을 직접 구해도 되지만 번거롭기 때문에 대개 이 값을 그대로 사용합니다.","metadata":{}},{"cell_type":"markdown","source":"### [5] Create datasets and data loaders (데이터셋 및 데이터 로더 생성)\n\nCreate training and validation datasets with the ImageDataset class. This is exactly the same as the baseline code, except for the transforms you pass in: when you create a training dataset, you pass in a transform for training, and when you create a validation dataset, you pass in a transform for validation/testing.\n\nImageDataset 클래스로 훈련 및 검증 데이터셋을 만듭니다. 전달하는 변환기를 제외하면 베이스라인 코드와 똑같습니다. 훈련 데이터셋을 만들 때는 훈련용 변환기를, 검증 데이터셋을 만들 때는 검증/테스트용 변환기를 전달합니다.","metadata":{}},{"cell_type":"code","source":"dataset_train = ImageDataset(df = train, img_dir = 'train/',\n                             transform = transform_train)\ndataset_valid = ImageDataset(df = valid, img_dir = 'train/',\n                             transform = transform_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:07.481449Z","iopub.execute_input":"2023-09-29T12:34:07.481762Z","iopub.status.idle":"2023-09-29T12:34:07.486547Z","shell.execute_reply.started":"2023-09-29T12:34:07.481731Z","shell.execute_reply":"2023-09-29T12:34:07.485671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader # 데이터 로더 클래스\n\nloader_train = DataLoader(dataset = dataset_train, batch_size = 32, shuffle = True)\nloader_valid = DataLoader(dataset = dataset_valid, batch_size = 32, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:07.487993Z","iopub.execute_input":"2023-09-29T12:34:07.488595Z","iopub.status.idle":"2023-09-29T12:34:07.500707Z","shell.execute_reply.started":"2023-09-29T12:34:07.488565Z","shell.execute_reply":"2023-09-29T12:34:07.499896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we created our dataset earlier, we passed in an image transform, which will perform an image transformation each time we load the data into the data loader. In this case, the transformers RandomHorizontalFlip(), RandomVerticalFlip(), and RandomRotation() will transform differently each time because they randomize the transformations. This means that the original image is the same, but you get the effect of training with different images in the first and second epochs. This is the \"data augmentation\" technique.\n\n앞서 데이터셋을 만들 때 이미지 변환기를 전달했습니다. 그러면 데이터 로더로 데이터를 불러올 때마다 이미지 변환을 수행합니다. 이때 변환기 중 RandomHorizontalFlip(), RandomVerticalFlip(), RandomRotation()은 변환을 무작위로 가하기 때문에 매번 다르게 변환합니다. 즉, 원본 이미지는 같지만 첫 번째 에폭과 두 번째 에폭에서 서로 다른 이미지로 훈련하는 효과를 얻을 수 있는 거죠. 이것이 바로 '데이터 증강' 기법입니다.","metadata":{}},{"cell_type":"markdown","source":"## 3. Create Model (모델 생성)\n\nNow that we have our data, let's design our CNN model. The baseline has two convolutional and two max-pooling layers, followed by a mean-pooling layer and a fully-connected layer.\n\nThis time, we're going to build a deeper CNN. Deeper neural network layers usually lead to better predictive power. However, be careful not to go too deep as it can lead to overfitting. We'll also apply batch normalization and change the activation function to Leaky ReLU for better performance.\n\nWe will also have a total of five layers {convolutional, batch normalization, max pooling} and two fully-connected layers.\n\nWe'll utilize `nn.Sequential()` to design the neural network layers.\n\n데이터가 준비되었으니 이제 CNN 모델을 설계해봅시다. 베이스라인에는 합성곱과 최대 풀링 계층이 두 개씩이고, 이어서 평균 플링 계층과 전결합 계층이 하나씩 있습니다.\n\n이번에는 더 깊은 CNN을 만들겠습니다. 신경망 계층이 깊어지면 대체로 예측력이 좋아집니다. 다만 지나치게 깊으면 과대적합될 우려가 있으니 유의하세요. 아울러 배치 정규화를 적용하고 활성화 함수를 Leaky ReLU로 바꿔서 성능을 더 높여보겠습니다.\n\n또한, {합성곱, 배치 정규화, 최대 풀링} 계층이 총 다섯 개에 전결합 계층도 두 개로 늘릴 것입니다.\n\n`nn.Sequential()`을 활용해 신경망 계층을 설계하겠습니다.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn # 신경망 모듈\nimport torch.nn.functional as F # 신경망 모듈에서 자주 사용되는 함수\n\nclass Model(nn.Module):\n    # 신경망 계층 정의\n    def __init__(self):\n        super().__init__() # 상속받은 nn.Module의 __init__() 메서드 호출\n        # 1~5번째 {합성곱, 배치 정규화, 최대 풀링} 계층\n        self.layer1 = nn.Sequential(nn.Conv2d(in_channels = 3, out_channels = 32,\n                                              kernel_size = 3, padding = 2),\n                                    nn.BatchNorm2d(32), # (1) 배치 정규화\n                                    nn.LeakyReLU(), # (2) LeakyReLu 활성화 함수\n                                    nn.MaxPool2d(kernel_size = 2))\n\n        self.layer2 = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 64,\n                                              kernel_size = 3, padding = 2),\n                                    nn.BatchNorm2d(64), # (1) 배치 정규화\n                                    nn.LeakyReLU(), # (2) LeakyReLu 활성화 함수\n                                    nn.MaxPool2d(kernel_size = 2))\n        \n        self.layer3 = nn.Sequential(nn.Conv2d(in_channels = 64, out_channels = 128,\n                                              kernel_size = 3, padding = 2),\n                                    nn.BatchNorm2d(128), # (1) 배치 정규화\n                                    nn.LeakyReLU(), # (2) LeakyReLu 활성화 함수\n                                    nn.MaxPool2d(kernel_size = 2))\n        \n        self.layer4 = nn.Sequential(nn.Conv2d(in_channels = 128, out_channels = 256,\n                                              kernel_size = 3, padding = 2),\n                                    nn.BatchNorm2d(256), # (1) 배치 정규화\n                                    nn.LeakyReLU(), # (2) LeakyReLu 활성화 함수\n                                    nn.MaxPool2d(kernel_size = 2))\n        \n        self.layer5 = nn.Sequential(nn.Conv2d(in_channels = 256, out_channels = 512,\n                                              kernel_size = 3, padding = 2),\n                                    nn.BatchNorm2d(512), # (1) 배치 정규화\n                                    nn.LeakyReLU(), # (2) LeakyReLu 활성화 함수\n                                    nn.MaxPool2d(kernel_size = 2))\n        \n        # 평균 풀링 계층\n        self.avg_pool = nn.AvgPool2d(kernel_size = 4)\n        # 전결합 계층\n        self.fc1 = nn.Linear(in_features = 512 * 1 * 1, out_features = 64)\n        self.fc2 = nn.Linear(in_features = 64, out_features = 2)\n    \n    # 순전파 출력 정의\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.avg_pool(x)\n        x = x.view(-1, 512 * 1 * 1) # 평탄화\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:07.502084Z","iopub.execute_input":"2023-09-29T12:34:07.502919Z","iopub.status.idle":"2023-09-29T12:34:07.514637Z","shell.execute_reply.started":"2023-09-29T12:34:07.502889Z","shell.execute_reply":"2023-09-29T12:34:07.513799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model().to(device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:34:07.517136Z","iopub.execute_input":"2023-09-29T12:34:07.517453Z","iopub.status.idle":"2023-09-29T12:34:12.294812Z","shell.execute_reply.started":"2023-09-29T12:34:07.51741Z","shell.execute_reply":"2023-09-29T12:34:12.293881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Train Model (모델 훈련)\n\nJust like in the baseline, we'll set up our loss function and optimizer and then start training.\n\n베이스라인 때와 마찬가지로 손실 함수와 옵티마이저를 설정한 후 훈련에 돌입하겠습니다.\n\n### Set Loss Function & Optimizer (손실 함수와 옵티마이저 설정)\n\nThe loss function will be CrossEntropyLoss(), the same as in the baseline.\n\n손실 함수는 베이스라인과 동일하게 CrossEntropyLoss()로 하겠습니다.","metadata":{}},{"cell_type":"code","source":"# 손실 함수\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:41:59.949708Z","iopub.execute_input":"2023-09-29T12:41:59.950027Z","iopub.status.idle":"2023-09-29T12:41:59.954319Z","shell.execute_reply.started":"2023-09-29T12:41:59.950004Z","shell.execute_reply":"2023-09-29T12:41:59.95333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's change the optimizer to Adamax (in the baseline, we used SGD, the default optimizer). You can think of Adamax as an improved version of Adam. Of course, Adamax doesn't always guarantee better results than SGD or Adam, and it's hard to tell which optimizer is better until you actually test it, so it's best to experiment.\n\n옵티마이저는 Adamax로 바꿔보겠습니다(베이스라인에서는 기본 옵티마이저인 SGD를 사용했습니다). Adamax는 Adam의 개선 버전이라고 보면 됩니다. 물론 Adamax가 SGD나 Adam보다 항상 나은 결과를 보장하는 건 아닙니다. 게다가 실제로 테스트해보기 전까진 어떤 옵티마이저가 더 좋은지 판단하기가 쉽지 않으니 여러 차례 실험을 해보는 게 좋습니다.","metadata":{}},{"cell_type":"code","source":"# 옵티마이저\noptimizer = torch.optim.Adamax(model.parameters(), lr = 0.00006)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:42:02.874332Z","iopub.execute_input":"2023-09-29T12:42:02.874672Z","iopub.status.idle":"2023-09-29T12:42:02.87889Z","shell.execute_reply.started":"2023-09-29T12:42:02.874646Z","shell.execute_reply":"2023-09-29T12:42:02.878033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model (모델 훈련)\n\nTo train more, we'll increase the number of epochs from 10 to 70, since we have more data to train with. The rest of the code is the same as the baseline.\n\n훈련을 더 많이 하기 위해 에폭 수를 10에서 70으로 늘리겠습니다. 데이터를 증강시켜 훈련할 데이터가 많아졌으니 에폭을 더 늘려도 되기 때문입니다. 나머지 코드는 베이스라인과 동일합니다.","metadata":{}},{"cell_type":"code","source":"epochs = 70 # 총 에폭\n# 총 에폭만큼 반복\nfor epoch in range(epochs):\n    epoch_loss = 0 # 에폭별 손실값 초기화\n    \n    # '반복 횟수'만큼 반복\n    for images, labels in loader_train:\n        # 이미지, 레이블 데이터 미니배치를 장비에 할당\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # 옵티마이저 내 기울기 초기화\n        optimizer.zero_grad()\n        # 순전파 : 이미지 데이터를 신경망 모델의 입력값으로 사용해 출력값 계산\n        outputs = model(images)\n        # 손실 함수를 활용해 outputs와 labels의 손실값 계산\n        loss = criterion(outputs, labels)\n        # 현재 배치에서의 손실 추가\n        epoch_loss += loss.item()\n        # 역전파 수행\n        loss.backward()\n        # 가중치 갱신\n        optimizer.step()\n        \n    # 훈련 데이터 손실값 출력\n    print(f'에폭 [{epoch + 1}/{epochs}] - 손실값 : {epoch_loss/len(loader_train):.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T12:44:23.708348Z","iopub.execute_input":"2023-09-29T12:44:23.708711Z","iopub.status.idle":"2023-09-29T13:25:32.156362Z","shell.execute_reply.started":"2023-09-29T12:44:23.708685Z","shell.execute_reply":"2023-09-29T13:25:32.155417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Validate Performance (성능 검증)\n\nLet's evaluate the model performance with validation data. Again, the code is no different from the baseline.\n\n검증 데이터로 모델 성능을 평가해보겠습니다. 역시 코드는 베이스라인과 다를 바 없습니다.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score # ROC AUC 점수 계산 함수 임포트\n# 실젯값과 예측 확률값을 담을 리스트 초기화\ntrue_list = []\npreds_list = []\n\nmodel.eval() # 모델을 평가 상태로 설정\n\nwith torch.no_grad(): # 기울기 계산 비활성화\n    for images, labels in loader_valid:\n        # 이미지, 레이블 데이터 미니배치를 장비에 할당\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # 순전파 : 이미지 데이터를 신경망 모델의 입력값으로 사용해 출력값 계산\n        outputs = model(images)\n        preds = torch.softmax(outputs.cpu(), dim = 1)[:, 1] # 예측 확률\n        true = labels.cpu() # 실젯값\n        # 예측 확률과 실젯값을 리스트에 추가\n        preds_list.extend(preds)\n        true_list.extend(true)\n        \n# 검증 데이터 ROC AUC 점수 계산\nprint(f'검증 데이터 ROC AUC : {roc_auc_score(true_list, preds_list):.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T13:37:56.112346Z","iopub.execute_input":"2023-09-29T13:37:56.112697Z","iopub.status.idle":"2023-09-29T13:37:58.614329Z","shell.execute_reply.started":"2023-09-29T13:37:56.112671Z","shell.execute_reply":"2023-09-29T13:37:58.613334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The baseline ROC AUC was 0.9900, but the refinements worked and it's now 0.9998. The maximum value of ROC AUC is 1, which is a near-perfect score. This means that we have almost perfect classification of the validation data.\n\n베이스라인의 ROC AUC는 0.9900이었는데, 개선 작업이 효과를 발휘해 0.9998이 되었네요. ROC AUC의 최댓값이 1이니 거의 완벽에 가까운 점수입니다. 검증 데이터를 거의 완벽히 분류해냈다는 뜻입니다.","metadata":{}},{"cell_type":"markdown","source":"## 6. Predict & Submission (예측 및 제출)\n\nNow it's time to make predictions with our test data. Again, we used the transform_test transformer to create our dataset.\n\n이제 테스트 데이터로 예측해봐야겠죠? 이번에도 transform_test 변환기를 이용해 데이터셋을 만들었습니다.","metadata":{}},{"cell_type":"code","source":"# 데이터셋과 데이터 로더 생성\ndataset_test = ImageDataset(df = submission, img_dir = 'test/',\n                            transform = transform_test)\nloader_test = DataLoader(dataset = dataset_test, batch_size = 32, shuffle = False)\n\n# 예측 수행\nmodel.eval() # 모델을 평가 상태로 설정\n\npreds = [] # 타깃 예측값 저장용 변수 초기화\n\nwith torch.no_grad(): # 기울기 계산 비활성화\n    for images, _ in loader_test:\n        # 이미지 데이터 미니배치를 장비에 할당\n        images = images.to(device)\n        \n        # 순전파 : 이미지 데이터를 신경망 모델의 입력값으로 사용해 출력값 계산\n        outputs = model(images)\n        # 타깃값이 1일 확률(예측값)\n        preds_part = torch.softmax(outputs.cpu(), dim = 1)[:, 1].tolist()\n        # preds에 preds_part 이어붙이기\n        preds.extend(preds_part)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T13:51:16.043161Z","iopub.execute_input":"2023-09-29T13:51:16.043512Z","iopub.status.idle":"2023-09-29T13:51:21.2585Z","shell.execute_reply.started":"2023-09-29T13:51:16.043484Z","shell.execute_reply":"2023-09-29T13:51:21.257571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a submission file and delete any directories you no longer need.\n\n제출 파일을 만들고 이제 필요 없는 디렉터리는 지워줍니다.","metadata":{}},{"cell_type":"code","source":"submission['has_cactus'] = preds\nsubmission.to_csv('submission.csv', index = False)\n\nimport shutil\n\nshutil.rmtree('./train')\nshutil.rmtree('./test')","metadata":{"execution":{"iopub.status.busy":"2023-09-29T13:52:05.912504Z","iopub.execute_input":"2023-09-29T13:52:05.912833Z","iopub.status.idle":"2023-09-29T13:52:06.607374Z","shell.execute_reply.started":"2023-09-29T13:52:05.912808Z","shell.execute_reply":"2023-09-29T13:52:06.606485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The End.","metadata":{}}]}